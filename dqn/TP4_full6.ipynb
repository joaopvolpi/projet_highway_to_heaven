{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e0c70d56",
      "metadata": {
        "id": "e0c70d56"
      },
      "source": [
        "# TD Deep Q-Network\n",
        "\n",
        "\n",
        "HÃ©di Hadiji March 2023  \n",
        "Adapted from Odalric Ambryn-Maillard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3192463f",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3192463f"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from copy import deepcopy\n",
        "import gymnasium as gym\n",
        "from tqdm import tqdm\n",
        "\n",
        "import time\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "EdfTNbNsBQY6",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EdfTNbNsBQY6"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8ea50e37",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8ea50e37",
        "outputId": "c90e510b-3290-423b-be21-f5a6bacdd5bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python --version = 3.9.18 (main, Sep 11 2023, 13:41:44) \n",
            "[GCC 11.2.0]\n",
            "torch.__version__ = 2.3.0+cu121\n",
            "np.__version__ = 1.22.4\n",
            "gym.__version__ = 0.29.1\n"
          ]
        }
      ],
      "source": [
        "print(f\"python --version = {sys.version}\")\n",
        "print(f\"torch.__version__ = {torch.__version__}\")\n",
        "print(f\"np.__version__ = {np.__version__}\")\n",
        "print(f\"gym.__version__ = {gym.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d7a8a0b1",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d7a8a0b1"
      },
      "outputs": [],
      "source": [
        "def run_one_episode(env, agent, display=True):\n",
        "    display_env = deepcopy(env)\n",
        "    done = False\n",
        "    state, _ = display_env.reset()\n",
        "\n",
        "    rewards = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.get_action(state, 0)\n",
        "        print(action)\n",
        "        state, reward, done, _, _ = display_env.step(action)\n",
        "        rewards += reward\n",
        "        if display:\n",
        "            clear_output(wait=True)\n",
        "            plt.imshow(display_env.render())\n",
        "            plt.show()\n",
        "    if display:\n",
        "        display_env.close()\n",
        "    print(f'Episode length {rewards}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "dbd61940",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dbd61940"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, reward, terminated, next_state):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = (state, action, reward, terminated, next_state)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.choices(self.memory, k=batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# create instance of replay buffer\n",
        "#replay_buffer = ReplayBuffer(BUFFER_CAPACITY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0decdfc3",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0decdfc3"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic neural net.\n",
        "    \"\"\"\n",
        "    def __init__(self, obs_size, hidden_size, n_actions):\n",
        "        super(Net, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x.to(device)).cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "47ae4de5",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "47ae4de5"
      },
      "outputs": [],
      "source": [
        "class DQN_Skeleton:\n",
        "    def __init__(self,\n",
        "                action_space,\n",
        "                observation_space,\n",
        "                gamma,\n",
        "                batch_size,\n",
        "                buffer_capacity,\n",
        "                update_target_every,\n",
        "                epsilon_start,\n",
        "                decrease_epsilon_factor,\n",
        "                epsilon_min,\n",
        "                learning_rate,\n",
        "                ):\n",
        "        self.action_space = action_space\n",
        "        self.observation_space = observation_space\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        self.update_target_every = update_target_every\n",
        "\n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.decrease_epsilon_factor = decrease_epsilon_factor # larger -> more exploration\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        ** TO BE IMPLEMENTED LATER**\n",
        "\n",
        "        Return action according to an epsilon-greedy exploration policy\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def update(self, *data):\n",
        "        \"\"\"\n",
        "        ** TO BE IMPLEMENTED LATER **\n",
        "\n",
        "        Updates the buffer and the network(s)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def get_q(self, state):\n",
        "        \"\"\"\n",
        "        Compute Q function for a states\n",
        "        \"\"\"\n",
        "\n",
        "        state = state.flatten()\n",
        "        state_tensor = torch.tensor(state).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            output = self.q_net.forward(state_tensor) # shape (1,  n_actions)\n",
        "        return output.numpy()[0]  # shape  (n_actions)\n",
        "\n",
        "    def decrease_epsilon(self):\n",
        "        self.epsilon = self.epsilon_min + (self.epsilon_start - self.epsilon_min) * (\n",
        "                        np.exp(-1. * self.n_eps / self.decrease_epsilon_factor ) )\n",
        "\n",
        "    def reset(self):\n",
        "        hidden_size = 256\n",
        "        obs_size = 1\n",
        "        for i in range(len(self.observation_space.shape)):\n",
        "          obs_size *= self.observation_space.shape[i]\n",
        "        n_actions = self.action_space.n\n",
        "\n",
        "        self.buffer = ReplayBuffer(self.buffer_capacity)\n",
        "        self.q_net =  Net(obs_size, hidden_size, n_actions).to(device)\n",
        "        self.target_net = Net(obs_size, hidden_size, n_actions).to(device)\n",
        "\n",
        "        self.loss_function = nn.MSELoss()\n",
        "        self.optimizer = optim.AdamW(params=self.q_net.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        self.epsilon = self.epsilon_start\n",
        "        self.n_steps = 0\n",
        "        self.n_eps = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "21ac7a03",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "21ac7a03"
      },
      "outputs": [],
      "source": [
        "class RandomAgent:\n",
        "    def __init__(self, observation_space, action_space):\n",
        "        self.action_space = action_space\n",
        "        return\n",
        "\n",
        "    def get_action(self, state, *args):\n",
        "        return self.action_space.sample()\n",
        "\n",
        "    def update(self, *data):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ae9698b8",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ae9698b8"
      },
      "outputs": [],
      "source": [
        "class DQN_SkeletonI(DQN_Skeleton):\n",
        "    def get_action(self, state, epsilon=None):\n",
        "        \"\"\"\n",
        "            ** Solution **\n",
        "\n",
        "            Return action according to an epsilon-greedy exploration policy\n",
        "        \"\"\"\n",
        "        if epsilon is None:\n",
        "            epsilon = self.epsilon\n",
        "\n",
        "        if np.random.rand() < epsilon:\n",
        "            return self.action_space.sample()\n",
        "        else:\n",
        "            return np.argmax(self.get_q(state))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9e15cae2",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9e15cae2"
      },
      "outputs": [],
      "source": [
        "def eval_agent(agent, env, n_sim=5):\n",
        "    \"\"\"\n",
        "    ** Solution **\n",
        "\n",
        "    Monte Carlo evaluation of DQN agent.\n",
        "\n",
        "    Repeat n_sim times:\n",
        "        * Run the DQN policy until the environment reaches a terminal state (= one episode)\n",
        "        * Compute the sum of rewards in this episode\n",
        "        * Store the sum of rewards in the episode_rewards array.\n",
        "    \"\"\"\n",
        "    env_copy = deepcopy(env)\n",
        "    episode_rewards = np.zeros(n_sim)\n",
        "    for i in range(n_sim):\n",
        "        state, _ = env_copy.reset()\n",
        "        reward_sum = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.get_action(state, 0)\n",
        "            state, reward, terminated, truncated, _ = env_copy.step(action)\n",
        "            reward_sum += reward\n",
        "            done = terminated or truncated\n",
        "        episode_rewards[i] = reward_sum\n",
        "    return episode_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f1226135",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f1226135"
      },
      "outputs": [],
      "source": [
        "class DQN(DQN_SkeletonI):\n",
        "    def update(self, state, action, reward, terminated, next_state):\n",
        "        \"\"\"\n",
        "        ** SOLUTION **\n",
        "        \"\"\"\n",
        "\n",
        "        # add data to replay buffer\n",
        "        self.buffer.push(torch.tensor(state).unsqueeze(0),\n",
        "                           torch.tensor([[action]], dtype=torch.int64),\n",
        "                           torch.tensor([reward]),\n",
        "                           torch.tensor([terminated], dtype=torch.int64),\n",
        "                           torch.tensor(next_state).unsqueeze(0),\n",
        "                          )\n",
        "\n",
        "        if len(self.buffer) < self.batch_size:\n",
        "            return np.inf\n",
        "\n",
        "        # get batch\n",
        "        transitions = self.buffer.sample(self.batch_size)\n",
        "\n",
        "        # Compute loss - TO BE IMPLEMENTED!\n",
        "        # Hint: use the gather method from torch.\n",
        "\n",
        "        state_batch, action_batch, reward_batch, terminated_batch, next_state_batch = tuple(\n",
        "            [torch.cat(data) for data in zip(*transitions)]\n",
        "        )\n",
        "        values  = self.q_net.forward(state_batch).gather(1, action_batch)\n",
        "\n",
        "        # Compute the ideal Q values\n",
        "        with torch.no_grad():\n",
        "            next_state_values = (1 - terminated_batch) * self.target_net(next_state_batch).max(1)[0]\n",
        "            targets = next_state_values * self.gamma + reward_batch\n",
        "\n",
        "        loss = self.loss_function(values, targets.unsqueeze(1).type(torch.float32))\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        #torch.nn.utils.clip_grad_value_(self.q_net.parameters(), 100)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if not((self.n_steps+1) % self.update_target_every):\n",
        "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "        self.decrease_epsilon()\n",
        "\n",
        "        self.n_steps += 1\n",
        "        if terminated:\n",
        "            self.n_eps += 1\n",
        "\n",
        "        return loss.detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "im-eWXPhB_ju",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "im-eWXPhB_ju"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\")\n",
        "\n",
        "config = {\n",
        "    \"observation\": {\n",
        "        \"type\": \"OccupancyGrid\",\n",
        "        \"vehicles_count\": 10,\n",
        "        \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\"],\n",
        "        \"features_range\": {\n",
        "            \"x\": [-100, 100],\n",
        "            \"y\": [-100, 100],\n",
        "            \"vx\": [-20, 20],\n",
        "            \"vy\": [-20, 20],\n",
        "        },\n",
        "        \"grid_size\": [[-20, 20], [-20, 20]],\n",
        "        \"grid_step\": [5, 5],\n",
        "        \"absolute\": False,\n",
        "        \"normalize\": True,\n",
        "    },\n",
        "    \"action\": {\n",
        "        \"type\": \"DiscreteAction\",\n",
        "    },\n",
        "    \"lanes_count\": 3,\n",
        "    \"vehicles_count\": 10,\n",
        "    \"duration\": 20,  # [s]\n",
        "    \"initial_spacing\": 0,\n",
        "    \"collision_reward\": -1,  # The reward received when colliding with a vehicle.\n",
        "    \"right_lane_reward\": 0.5,  # The reward received when driving on the right-most lanes, linearly mapped to\n",
        "    # zero for other lanes.\n",
        "    \"high_speed_reward\": 0.1,  # The reward received when driving at full speed, linearly mapped to zero for\n",
        "    # lower speeds according to config[\"reward_speed_range\"].\n",
        "    \"lane_change_reward\": 0,\n",
        "    \"reward_speed_range\": [\n",
        "        20,\n",
        "        30,\n",
        "    ],  # [m/s] The reward for high speed is mapped linearly from this range to [0, HighwayEnv.HIGH_SPEED_REWARD].\n",
        "    \"simulation_frequency\": 5,  # [Hz]\n",
        "    \"policy_frequency\": 1,  # [Hz]\n",
        "    \"other_vehicles_type\": \"highway_env.vehicle.behavior.IDMVehicle\",\n",
        "    \"screen_width\": 600,  # [px]\n",
        "    \"screen_height\": 150,  # [px]\n",
        "    \"centering_position\": [0.3, 0.5],\n",
        "    \"scaling\": 5.5,\n",
        "    \"show_trajectories\": True,\n",
        "    \"render_agent\": True,\n",
        "    \"offscreen_rendering\": False,\n",
        "    \"disable_collision_checks\": True,\n",
        "}\n",
        "\n",
        "env.unwrapped.configure(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "920b7751",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "920b7751"
      },
      "outputs": [],
      "source": [
        "def train(env, agent, N_episodes, eval_every=10, reward_threshold=300):\n",
        "    total_time = 0\n",
        "    state, _ = env.reset()\n",
        "    losses = []\n",
        "    for ep in range(N_episodes):\n",
        "        done = False\n",
        "        state, _ = env.reset()\n",
        "        state = state.flatten()  # Flattening the state at reset for each episode\n",
        "        while not done:\n",
        "            action = agent.get_action(state, agent.epsilon)  # Make sure to pass epsilon for exploration\n",
        "            #,action = agent.get_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            next_state = next_state.flatten().astype(np.float32)  # Flattening next_state from the environment\n",
        "            loss_val = agent.update(state, action, reward, terminated, next_state)\n",
        "            state = next_state\n",
        "            losses.append(loss_val)\n",
        "            done = terminated or truncated\n",
        "            total_time += 1\n",
        "\n",
        "\n",
        "\n",
        "        if ((ep+1)% eval_every == 0):\n",
        "            rewards = eval_agent(agent, env)\n",
        "            print(\"episode =\", ep+1, \", reward = \", np.mean(rewards), \", loss = \", loss_val)\n",
        "            if np.mean(rewards) >= reward_threshold:\n",
        "                break\n",
        "\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c31c46a7",
      "metadata": {},
      "source": [
        "gamma changed 0.6 0.8 0.99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ziKge5LNpufB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziKge5LNpufB",
        "outputId": "03c91ba4-26e3-43db-bdea-a51f59026c8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/vitoriano/anaconda3/envs/RL/lib/python3.9/site-packages/torch/autograd/graph.py:744: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode = 10 , reward =  6.78125 , loss =  0.0880568\n",
            "episode = 20 , reward =  6.5625 , loss =  0.06831971\n",
            "episode = 30 , reward =  0.1875 , loss =  0.09202192\n",
            "episode = 40 , reward =  10.905938185837204 , loss =  0.12087237\n",
            "episode = 50 , reward =  4.680438164185896 , loss =  0.09699739\n",
            "episode = 60 , reward =  3.225 , loss =  0.10683595\n",
            "episode = 70 , reward =  3.6130264730679187 , loss =  0.11833271\n",
            "episode = 80 , reward =  4.3875 , loss =  0.075116985\n",
            "episode = 90 , reward =  7.75625 , loss =  0.11502486\n",
            "episode = 100 , reward =  10.71935 , loss =  0.110110596\n",
            "episode = 110 , reward =  6.4514 , loss =  0.16083892\n",
            "episode = 120 , reward =  6.98125 , loss =  0.11974309\n",
            "episode = 130 , reward =  15.45 , loss =  0.09278284\n",
            "episode = 140 , reward =  6.76875 , loss =  0.09561393\n",
            "episode = 150 , reward =  4.44375 , loss =  0.08185652\n",
            "episode = 160 , reward =  4.863913999999999 , loss =  0.10912801\n",
            "episode = 170 , reward =  5.8625 , loss =  0.15232553\n",
            "episode = 180 , reward =  8.169862 , loss =  0.16748992\n",
            "episode = 190 , reward =  9.80475399573577 , loss =  0.23831603\n",
            "episode = 200 , reward =  5.3875 , loss =  0.14125282\n",
            "episode = 210 , reward =  8.98125 , loss =  0.14141704\n",
            "episode = 220 , reward =  9.285250000000001 , loss =  0.15701151\n",
            "episode = 230 , reward =  6.40625 , loss =  0.14814682\n",
            "episode = 240 , reward =  10.75 , loss =  0.1547927\n",
            "episode = 250 , reward =  7.284000000000001 , loss =  0.1911428\n",
            "episode = 260 , reward =  3.44885 , loss =  0.21699294\n",
            "episode = 270 , reward =  7.5125 , loss =  0.18817465\n",
            "episode = 280 , reward =  4.043 , loss =  0.16361375\n",
            "episode = 290 , reward =  3.080050797742806 , loss =  0.1740554\n",
            "episode = 300 , reward =  3.1874000000000002 , loss =  0.13829607\n",
            "episode = 310 , reward =  4.809418571563272 , loss =  0.20098922\n",
            "episode = 320 , reward =  3.7571507977428054 , loss =  0.27819303\n",
            "episode = 330 , reward =  3.7563465645975724 , loss =  0.2031917\n",
            "episode = 340 , reward =  14.6375 , loss =  0.2198509\n",
            "episode = 350 , reward =  4.825076687458437 , loss =  0.33970475\n",
            "episode = 360 , reward =  4.15625 , loss =  0.20484516\n",
            "episode = 370 , reward =  4.597908280480675 , loss =  0.34920058\n",
            "episode = 380 , reward =  5.9565 , loss =  0.24777743\n",
            "episode = 390 , reward =  2.6614 , loss =  0.34215358\n",
            "episode = 400 , reward =  11.2625 , loss =  0.5019892\n",
            "episode = 410 , reward =  11.80625 , loss =  0.20110518\n",
            "episode = 420 , reward =  10.875 , loss =  0.30448937\n",
            "episode = 430 , reward =  13.04375 , loss =  0.25799832\n",
            "episode = 440 , reward =  14.425 , loss =  0.2681171\n",
            "episode = 450 , reward =  14.35 , loss =  0.17788373\n",
            "episode = 460 , reward =  5.287875491022639 , loss =  0.2079474\n",
            "episode = 470 , reward =  4.3340000000000005 , loss =  0.3099895\n",
            "episode = 480 , reward =  13.9375 , loss =  0.29292583\n",
            "episode = 490 , reward =  13.2 , loss =  0.36683074\n",
            "episode = 500 , reward =  6.876399999999999 , loss =  0.38841158\n",
            "episode = 510 , reward =  11.23125 , loss =  0.37485904\n",
            "episode = 520 , reward =  16.375 , loss =  0.3059429\n",
            "episode = 530 , reward =  14.325 , loss =  0.5458145\n",
            "episode = 540 , reward =  11.88125 , loss =  0.37694356\n",
            "episode = 550 , reward =  10.0875 , loss =  0.28918144\n",
            "episode = 560 , reward =  13.7 , loss =  0.26880935\n",
            "episode = 570 , reward =  12.04375 , loss =  0.22673789\n",
            "episode = 580 , reward =  14.4665 , loss =  0.34172162\n",
            "episode = 590 , reward =  7.407649999999999 , loss =  0.41046154\n",
            "episode = 600 , reward =  11.73125 , loss =  0.73307526\n",
            "episode = 610 , reward =  10.5 , loss =  0.4548512\n",
            "episode = 620 , reward =  7.7646500000000005 , loss =  0.32099503\n",
            "episode = 630 , reward =  9.395520963241102 , loss =  0.31154084\n",
            "episode = 640 , reward =  12.7875 , loss =  0.31186187\n",
            "episode = 650 , reward =  11.2625 , loss =  0.3821483\n",
            "episode = 660 , reward =  8.43365 , loss =  0.40686426\n",
            "episode = 670 , reward =  14.475 , loss =  0.3023274\n",
            "episode = 680 , reward =  13.25625 , loss =  0.2753555\n",
            "episode = 690 , reward =  15.26875 , loss =  0.6253437\n",
            "episode = 700 , reward =  11.5375 , loss =  0.30311376\n",
            "episode = 710 , reward =  9.63265 , loss =  0.27601522\n",
            "episode = 720 , reward =  9.04025 , loss =  0.5373038\n",
            "episode = 730 , reward =  11.38125 , loss =  0.19153176\n",
            "episode = 740 , reward =  6.997749999999999 , loss =  0.55868524\n",
            "episode = 750 , reward =  9.138749999999998 , loss =  0.67880565\n",
            "episode = 760 , reward =  12.71875 , loss =  0.4063018\n",
            "episode = 770 , reward =  9.9875 , loss =  0.61262476\n",
            "episode = 780 , reward =  11.475 , loss =  0.4911041\n",
            "episode = 790 , reward =  4.49375 , loss =  0.608543\n",
            "episode = 800 , reward =  9.291469216511487 , loss =  0.56201696\n",
            "episode = 810 , reward =  10.0625 , loss =  0.48347566\n",
            "episode = 820 , reward =  12.975 , loss =  0.6424175\n",
            "episode = 830 , reward =  15.7625 , loss =  0.45251024\n",
            "episode = 840 , reward =  12.45 , loss =  0.44785532\n",
            "episode = 850 , reward =  17.025 , loss =  0.29617384\n",
            "episode = 860 , reward =  13.30377257443079 , loss =  0.5130236\n",
            "episode = 870 , reward =  11.15625 , loss =  0.82133335\n",
            "episode = 880 , reward =  13.0625 , loss =  0.6536378\n",
            "episode = 890 , reward =  14.15625 , loss =  0.4158376\n",
            "episode = 900 , reward =  16.375 , loss =  0.8944576\n",
            "episode = 910 , reward =  14.332650000000001 , loss =  0.79754627\n",
            "episode = 920 , reward =  13.8625 , loss =  1.3221413\n",
            "episode = 930 , reward =  7.0269 , loss =  0.713017\n",
            "episode = 940 , reward =  14.43125 , loss =  1.0733185\n",
            "episode = 950 , reward =  13.39375 , loss =  1.9406589\n",
            "episode = 960 , reward =  15.075 , loss =  0.8188844\n",
            "episode = 970 , reward =  15.05 , loss =  4.4947014\n",
            "episode = 980 , reward =  15.775 , loss =  1.9686742\n",
            "episode = 990 , reward =  14.38585 , loss =  2.2874107\n",
            "episode = 1000 , reward =  13.79375 , loss =  2.4241014\n",
            "episode = 1010 , reward =  15.675 , loss =  2.3625414\n",
            "episode = 1020 , reward =  16.93125 , loss =  1.2440337\n",
            "episode = 1030 , reward =  16.95 , loss =  3.2770324\n",
            "episode = 1040 , reward =  15.71875 , loss =  4.103828\n",
            "episode = 1050 , reward =  14.5125 , loss =  4.178027\n",
            "episode = 1060 , reward =  16.35 , loss =  4.519739\n",
            "episode = 1070 , reward =  14.6 , loss =  2.1223168\n",
            "episode = 1080 , reward =  13.9125 , loss =  4.188521\n",
            "episode = 1090 , reward =  14.03125 , loss =  4.5838447\n",
            "episode = 1100 , reward =  11.58125 , loss =  3.8004017\n",
            "episode = 1110 , reward =  13.83125 , loss =  5.011097\n",
            "episode = 1120 , reward =  15.09375 , loss =  6.844749\n",
            "episode = 1130 , reward =  10.9375 , loss =  2.0816367\n",
            "episode = 1140 , reward =  17.55 , loss =  2.5394254\n",
            "episode = 1150 , reward =  15.0875 , loss =  5.05633\n",
            "episode = 1160 , reward =  16.9625 , loss =  2.8000896\n",
            "episode = 1170 , reward =  14.43125 , loss =  6.15822\n",
            "episode = 1180 , reward =  14.45 , loss =  4.6915236\n",
            "episode = 1190 , reward =  14.924501331911923 , loss =  3.8193069\n",
            "episode = 1200 , reward =  14.45 , loss =  2.7471035\n",
            "episode = 1210 , reward =  14.45 , loss =  4.782909\n",
            "episode = 1220 , reward =  13.81875 , loss =  4.0173535\n",
            "episode = 1230 , reward =  15.10625 , loss =  6.7741723\n",
            "episode = 1240 , reward =  16.3 , loss =  9.34254\n",
            "episode = 1250 , reward =  14.43125 , loss =  4.8653817\n",
            "episode = 1260 , reward =  12.94375 , loss =  7.364223\n",
            "episode = 1270 , reward =  11.375 , loss =  1.8193177\n",
            "episode = 1280 , reward =  9.625 , loss =  2.5931122\n",
            "episode = 1290 , reward =  10.44375 , loss =  9.667251\n",
            "episode = 1300 , reward =  11.375 , loss =  4.0360794\n",
            "episode = 1310 , reward =  13.16875 , loss =  4.4545474\n",
            "episode = 1320 , reward =  14.2875 , loss =  7.554411\n",
            "episode = 1330 , reward =  14.4625 , loss =  6.549548\n",
            "episode = 1340 , reward =  16.3125 , loss =  8.101683\n",
            "episode = 1350 , reward =  14.475 , loss =  2.581953\n",
            "episode = 1360 , reward =  16.30625 , loss =  5.9639344\n",
            "episode = 1370 , reward =  10.5 , loss =  7.480937\n",
            "episode = 1380 , reward =  10.9375 , loss =  10.393941\n",
            "episode = 1390 , reward =  15.68125 , loss =  11.240631\n",
            "episode = 1400 , reward =  10.3 , loss =  3.9743965\n",
            "episode = 1410 , reward =  11.375 , loss =  6.458341\n",
            "episode = 1420 , reward =  15.70625 , loss =  5.9705443\n",
            "episode = 1430 , reward =  15.0625 , loss =  10.1468\n",
            "episode = 1440 , reward =  10.9375 , loss =  5.1602683\n",
            "episode = 1450 , reward =  16.93125 , loss =  6.6730933\n",
            "episode = 1460 , reward =  11.8125 , loss =  8.466533\n",
            "episode = 1470 , reward =  15.6875 , loss =  4.6615887\n",
            "episode = 1480 , reward =  16.9375 , loss =  6.947435\n",
            "episode = 1490 , reward =  16.34375 , loss =  6.036946\n",
            "episode = 1500 , reward =  16.33125 , loss =  8.842721\n",
            "episode = 1510 , reward =  15.6875 , loss =  11.851405\n",
            "episode = 1520 , reward =  15.08125 , loss =  26.45789\n",
            "episode = 1530 , reward =  15.7 , loss =  23.06336\n",
            "episode = 1540 , reward =  16.34375 , loss =  29.113306\n",
            "episode = 1550 , reward =  14.43125 , loss =  13.095337\n",
            "episode = 1560 , reward =  16.95 , loss =  10.55328\n",
            "episode = 1570 , reward =  13.6125 , loss =  11.437235\n",
            "episode = 1580 , reward =  15.05 , loss =  13.002728\n",
            "episode = 1590 , reward =  16.3125 , loss =  12.766409\n",
            "episode = 1600 , reward =  14.4375 , loss =  47.100975\n",
            "episode = 1610 , reward =  14.43125 , loss =  17.28736\n",
            "episode = 1620 , reward =  16.3 , loss =  67.166374\n",
            "episode = 1630 , reward =  13.80625 , loss =  26.974836\n",
            "episode = 1640 , reward =  15.68125 , loss =  26.089525\n",
            "episode = 1650 , reward =  16.9375 , loss =  19.133276\n",
            "episode = 1660 , reward =  15.1125 , loss =  14.357345\n",
            "episode = 1670 , reward =  16.93125 , loss =  57.2809\n",
            "episode = 1680 , reward =  16.93125 , loss =  23.65379\n",
            "episode = 1690 , reward =  16.325 , loss =  38.40471\n",
            "episode = 1700 , reward =  16.30625 , loss =  77.50383\n",
            "episode = 1710 , reward =  16.93125 , loss =  22.438122\n",
            "episode = 1720 , reward =  15.81875 , loss =  29.382404\n",
            "episode = 1730 , reward =  15.0625 , loss =  68.79931\n",
            "episode = 1740 , reward =  16.31875 , loss =  44.557484\n",
            "episode = 1750 , reward =  15.75 , loss =  71.95637\n",
            "episode = 1760 , reward =  13.8375 , loss =  64.97332\n",
            "episode = 1770 , reward =  16.91875 , loss =  29.1983\n",
            "episode = 1780 , reward =  14.934825398871402 , loss =  86.61029\n",
            "episode = 1790 , reward =  16.30625 , loss =  35.114117\n",
            "episode = 1800 , reward =  13.897325398871402 , loss =  302.0561\n",
            "episode = 1810 , reward =  15.05 , loss =  65.83133\n",
            "episode = 1820 , reward =  13.35625 , loss =  105.26601\n",
            "episode = 1830 , reward =  15.8 , loss =  147.35258\n",
            "episode = 1840 , reward =  15.10625 , loss =  116.86225\n",
            "episode = 1850 , reward =  15.6875 , loss =  65.88745\n",
            "episode = 1860 , reward =  15.0875 , loss =  151.24944\n",
            "episode = 1870 , reward =  13.90625 , loss =  102.19774\n",
            "episode = 1880 , reward =  15.06875 , loss =  200.56987\n",
            "episode = 1890 , reward =  11.51085 , loss =  185.79861\n",
            "episode = 1900 , reward =  14.84675 , loss =  76.1875\n",
            "episode = 1910 , reward =  16.3 , loss =  130.71454\n",
            "episode = 1920 , reward =  12.78125 , loss =  128.63292\n",
            "episode = 1930 , reward =  16.9875 , loss =  191.48787\n",
            "episode = 1940 , reward =  16.91875 , loss =  78.50677\n",
            "episode = 1950 , reward =  16.29375 , loss =  242.4613\n",
            "episode = 1960 , reward =  13.11875 , loss =  157.29173\n",
            "episode = 1970 , reward =  15.68125 , loss =  145.68715\n",
            "episode = 1980 , reward =  15.68125 , loss =  130.18703\n",
            "episode = 1990 , reward =  13.5625 , loss =  174.3846\n",
            "episode = 2000 , reward =  16.4 , loss =  103.21318\n",
            "episode = 2010 , reward =  15.675 , loss =  88.26672\n",
            "episode = 2020 , reward =  15.09375 , loss =  99.04995\n",
            "episode = 2030 , reward =  13.2375 , loss =  144.64618\n",
            "episode = 2040 , reward =  12.825 , loss =  132.81361\n",
            "episode = 2050 , reward =  16.3375 , loss =  254.8354\n",
            "episode = 2060 , reward =  13.125 , loss =  136.67593\n",
            "episode = 2070 , reward =  13.175 , loss =  75.997086\n",
            "episode = 2080 , reward =  13.55 , loss =  79.68099\n",
            "episode = 2090 , reward =  15.06875 , loss =  138.25867\n",
            "episode = 2100 , reward =  12.31875 , loss =  80.264824\n",
            "episode = 2110 , reward =  15.6125 , loss =  77.80413\n",
            "episode = 2120 , reward =  10.025 , loss =  86.06494\n",
            "episode = 2130 , reward =  14.35625 , loss =  91.86279\n",
            "episode = 2140 , reward =  14.4875 , loss =  109.8123\n",
            "episode = 2150 , reward =  15.05625 , loss =  120.26534\n",
            "episode = 2160 , reward =  18.1625 , loss =  129.49387\n",
            "episode = 2170 , reward =  12.375 , loss =  48.936394\n",
            "episode = 2180 , reward =  16.33125 , loss =  134.04263\n",
            "episode = 2190 , reward =  14.46875 , loss =  33.233414\n",
            "episode = 2200 , reward =  16.30625 , loss =  109.088776\n",
            "episode = 2210 , reward =  16.33125 , loss =  58.0699\n",
            "episode = 2220 , reward =  15.1 , loss =  289.94012\n",
            "episode = 2230 , reward =  16.95625 , loss =  37.265057\n",
            "episode = 2240 , reward =  15.68125 , loss =  120.70328\n",
            "episode = 2250 , reward =  17.58125 , loss =  55.038322\n",
            "episode = 2260 , reward =  9.21875 , loss =  58.399456\n",
            "episode = 2270 , reward =  16.95 , loss =  49.808483\n",
            "episode = 2280 , reward =  15.0875 , loss =  81.84999\n",
            "episode = 2290 , reward =  15.675 , loss =  80.82938\n",
            "episode = 2300 , reward =  11.9 , loss =  49.03789\n",
            "episode = 2310 , reward =  10.36875 , loss =  46.311405\n",
            "episode = 2320 , reward =  10.44375 , loss =  38.625816\n",
            "episode = 2330 , reward =  13.7 , loss =  56.26431\n",
            "episode = 2340 , reward =  15.6875 , loss =  26.579285\n",
            "episode = 2350 , reward =  13.88125 , loss =  22.75192\n",
            "episode = 2360 , reward =  15.75 , loss =  52.274536\n",
            "episode = 2370 , reward =  17.5625 , loss =  36.163277\n",
            "episode = 2380 , reward =  10.8625 , loss =  58.047104\n",
            "episode = 2390 , reward =  12.9 , loss =  22.426159\n",
            "episode = 2400 , reward =  13.11875 , loss =  31.701265\n",
            "episode = 2410 , reward =  15.4375 , loss =  18.583742\n",
            "episode = 2420 , reward =  13.9125 , loss =  22.804325\n",
            "episode = 2430 , reward =  12.05625 , loss =  25.554081\n",
            "episode = 2440 , reward =  14.3125 , loss =  11.815884\n",
            "episode = 2450 , reward =  10.9375 , loss =  19.824036\n",
            "episode = 2460 , reward =  9.375 , loss =  9.746942\n",
            "episode = 2470 , reward =  12.193151598294309 , loss =  11.039418\n",
            "episode = 2480 , reward =  13.98125 , loss =  24.576319\n",
            "episode = 2490 , reward =  7.3985 , loss =  11.352663\n",
            "episode = 2500 , reward =  15.05625 , loss =  19.133198\n",
            "episode = 2510 , reward =  13.88125 , loss =  28.861162\n",
            "episode = 2520 , reward =  13.8875 , loss =  26.00581\n",
            "episode = 2530 , reward =  15.13125 , loss =  22.377838\n",
            "episode = 2540 , reward =  15.50625 , loss =  28.243904\n",
            "episode = 2550 , reward =  14.71875 , loss =  93.22957\n",
            "episode = 2560 , reward =  15.5625 , loss =  14.011745\n",
            "episode = 2570 , reward =  14.73125 , loss =  12.380448\n",
            "episode = 2580 , reward =  12.79375 , loss =  10.939374\n",
            "episode = 2590 , reward =  17.58125 , loss =  20.090954\n",
            "episode = 2600 , reward =  6.35625 , loss =  30.375969\n",
            "episode = 2610 , reward =  16.99375 , loss =  25.798412\n",
            "episode = 2620 , reward =  11.64375 , loss =  9.832053\n",
            "episode = 2630 , reward =  15.13125 , loss =  13.920639\n",
            "episode = 2640 , reward =  15.075 , loss =  17.100014\n",
            "episode = 2650 , reward =  11.24875 , loss =  13.280109\n",
            "episode = 2660 , reward =  13.8125 , loss =  216.44894\n",
            "episode = 2670 , reward =  16.99375 , loss =  13.359437\n",
            "episode = 2680 , reward =  15.69375 , loss =  28.36437\n",
            "episode = 2690 , reward =  9.960505327647693 , loss =  5.2718506\n",
            "episode = 2700 , reward =  11.743344082092948 , loss =  81.22701\n",
            "episode = 2710 , reward =  9.4375 , loss =  22.25064\n",
            "episode = 2720 , reward =  14.15 , loss =  11.7881565\n",
            "episode = 2730 , reward =  18.21875 , loss =  20.944098\n",
            "episode = 2740 , reward =  14.2165 , loss =  123.48679\n",
            "episode = 2750 , reward =  15.71875 , loss =  12.573089\n",
            "episode = 2760 , reward =  12.51875 , loss =  3.6718118\n",
            "episode = 2770 , reward =  9.94375 , loss =  15.376002\n",
            "episode = 2780 , reward =  14.5125 , loss =  50.04636\n",
            "episode = 2790 , reward =  12.049594082092948 , loss =  8.050755\n",
            "episode = 2800 , reward =  12.21875 , loss =  28.050205\n",
            "episode = 2810 , reward =  10.5 , loss =  34.38848\n",
            "episode = 2820 , reward =  10.9375 , loss =  12.485692\n",
            "episode = 2830 , reward =  11.8125 , loss =  7.615053\n",
            "episode = 2840 , reward =  15.00625 , loss =  1.7157099\n",
            "episode = 2850 , reward =  10.0625 , loss =  6.4225683\n",
            "episode = 2860 , reward =  9.808270362438938 , loss =  14.959426\n",
            "episode = 2870 , reward =  13.17175 , loss =  2.2029004\n",
            "episode = 2880 , reward =  9.674594082092948 , loss =  15.982398\n",
            "episode = 2890 , reward =  15.4125 , loss =  15.361865\n",
            "episode = 2900 , reward =  11.93125 , loss =  12.756942\n",
            "episode = 2910 , reward =  10.8 , loss =  9.203671\n",
            "episode = 2920 , reward =  16.96875 , loss =  14.419168\n",
            "episode = 2930 , reward =  8.9875 , loss =  27.586433\n",
            "episode = 2940 , reward =  11.99375 , loss =  308.77692\n",
            "episode = 2950 , reward =  15.38125 , loss =  18.407778\n",
            "episode = 2960 , reward =  16.3125 , loss =  21.132938\n",
            "episode = 2970 , reward =  16.3125 , loss =  10.444218\n",
            "episode = 2980 , reward =  13.7375 , loss =  13.219252\n",
            "episode = 2990 , reward =  14.425 , loss =  7.0794206\n",
            "episode = 3000 , reward =  15.0625 , loss =  23.292519\n",
            "\n",
            "mean reward after training =  14.451188498933941\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "action_space = env.action_space\n",
        "observation_space = env.observation_space\n",
        "\n",
        "batch_size = 128\n",
        "buffer_capacity = 10_000\n",
        "update_target_every = 32\n",
        "\n",
        "gamma = 0.99\n",
        "batch_size = 128\n",
        "buffer_capacity = 10_000\n",
        "update_target_every = 512\n",
        "\n",
        "epsilon_start = 0.99\n",
        "decrease_epsilon_factor = 1000 # 300\n",
        "epsilon_min = 0.03\n",
        "\n",
        "learning_rate = 1e-3 \n",
        "\n",
        "arguments = (action_space,\n",
        "            observation_space,\n",
        "            gamma,\n",
        "            batch_size,\n",
        "            buffer_capacity,\n",
        "            update_target_every,\n",
        "            epsilon_start,\n",
        "            decrease_epsilon_factor,\n",
        "            epsilon_min,\n",
        "            learning_rate,\n",
        "        )\n",
        "\n",
        "N_episodes = int(3e3)\n",
        "\n",
        "agent = DQN(*arguments)\n",
        "\n",
        "\n",
        "# Run the training loop\n",
        "losses = train(env, agent, N_episodes)\n",
        "\n",
        "plt.plot(losses)\n",
        "\n",
        "# Evaluate the final policy\n",
        "rewards = eval_agent(agent, env, 20)\n",
        "print(\"\")\n",
        "print(\"mean reward after training = \", np.mean(rewards))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d274ea71",
      "metadata": {},
      "source": [
        "mean reward after training =  5.3150470515232175  gamma = 0.6 n normalized\n",
        "mean reward after training =  8.436077401778164 gamma = 0.99 n normalized\n",
        "mean reward after training =  6.027701038379542 gamma = 0.8 n normalized\n",
        "\n",
        "mean reward after training =  14.451188498933941 gamma = 0.99 normalized\n",
        "mean reward after training =  4.447856334147227 gamma = 0.6 normalized\n",
        "mean reward after training =  3.5761701566076285 gamma = 0.8 normalized\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "898bfde9",
      "metadata": {
        "id": "898bfde9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rewards after training =  [18.78125 12.59375 15.6875  15.6875  12.5625 ]\n"
          ]
        }
      ],
      "source": [
        "print(\"Rewards after training = \", eval_agent(agent, env))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "P3FixzsyBj7a",
      "metadata": {
        "id": "P3FixzsyBj7a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'get_image'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_one_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mrun_one_episode\u001b[0;34m(env, agent, display)\u001b[0m\n\u001b[1;32m      9\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(state, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(action)\n\u001b[0;32m---> 11\u001b[0m state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdisplay_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m display:\n",
            "File \u001b[0;32m~/anaconda3/envs/RL/lib/python3.9/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/RL/lib/python3.9/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/RL/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[1;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
            "File \u001b[0;32m~/anaconda3/envs/RL/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:264\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frame \u001b[38;5;241m<\u001b[39m frames \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# Last frame will be rendered through env.render() as usual\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_automatic_rendering\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_auto_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/RL/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:325\u001b[0m, in \u001b[0;36mAbstractEnv._automatic_rendering\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_video_wrapper\u001b[38;5;241m.\u001b[39mvideo_recorder\u001b[38;5;241m.\u001b[39mcapture_frame()\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/RL/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:292\u001b[0m, in \u001b[0;36mAbstractEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39mhandle_events()\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 292\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image\u001b[49m()\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_image'"
          ]
        }
      ],
      "source": [
        "run_one_episode(env, agent, display=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ce351865",
      "metadata": {},
      "outputs": [],
      "source": [
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
