{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e0c70d56",
      "metadata": {
        "id": "e0c70d56"
      },
      "source": [
        "# TD Deep Q-Network\n",
        "\n",
        "\n",
        "HÃ©di Hadiji March 2023  \n",
        "Adapted from Odalric Ambryn-Maillard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3192463f",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3192463f"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from copy import deepcopy\n",
        "import gymnasium as gym\n",
        "from tqdm import tqdm\n",
        "\n",
        "import time\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EdfTNbNsBQY6",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EdfTNbNsBQY6"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ea50e37",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8ea50e37",
        "outputId": "c90e510b-3290-423b-be21-f5a6bacdd5bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python --version = 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "torch.__version__ = 2.2.1+cu121\n",
            "np.__version__ = 1.25.2\n",
            "gym.__version__ = 0.29.1\n"
          ]
        }
      ],
      "source": [
        "print(f\"python --version = {sys.version}\")\n",
        "print(f\"torch.__version__ = {torch.__version__}\")\n",
        "print(f\"np.__version__ = {np.__version__}\")\n",
        "print(f\"gym.__version__ = {gym.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7a8a0b1",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d7a8a0b1"
      },
      "outputs": [],
      "source": [
        "def run_one_episode(env, agent, display=True):\n",
        "    display_env = deepcopy(env)\n",
        "    done = False\n",
        "    state, _ = display_env.reset()\n",
        "\n",
        "    rewards = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.get_action(state, 0)\n",
        "        print(action)\n",
        "        state, reward, done, _, _ = display_env.step(action)\n",
        "        rewards += reward\n",
        "        if display:\n",
        "            clear_output(wait=True)\n",
        "            plt.imshow(display_env.render())\n",
        "            plt.show()\n",
        "    if display:\n",
        "        display_env.close()\n",
        "    print(f'Episode length {rewards}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbd61940",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dbd61940"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, reward, terminated, next_state):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = (state, action, reward, terminated, next_state)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.choices(self.memory, k=batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# create instance of replay buffer\n",
        "#replay_buffer = ReplayBuffer(BUFFER_CAPACITY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0decdfc3",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0decdfc3"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic neural net.\n",
        "    \"\"\"\n",
        "    def __init__(self, obs_size, hidden_size, n_actions):\n",
        "        super(Net, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x.to(device)).cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47ae4de5",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "47ae4de5"
      },
      "outputs": [],
      "source": [
        "class DQN_Skeleton:\n",
        "    def __init__(self,\n",
        "                action_space,\n",
        "                observation_space,\n",
        "                gamma,\n",
        "                batch_size,\n",
        "                buffer_capacity,\n",
        "                update_target_every,\n",
        "                epsilon_start,\n",
        "                decrease_epsilon_factor,\n",
        "                epsilon_min,\n",
        "                learning_rate,\n",
        "                ):\n",
        "        self.action_space = action_space\n",
        "        self.observation_space = observation_space\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        self.update_target_every = update_target_every\n",
        "\n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.decrease_epsilon_factor = decrease_epsilon_factor # larger -> more exploration\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        ** TO BE IMPLEMENTED LATER**\n",
        "\n",
        "        Return action according to an epsilon-greedy exploration policy\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def update(self, *data):\n",
        "        \"\"\"\n",
        "        ** TO BE IMPLEMENTED LATER **\n",
        "\n",
        "        Updates the buffer and the network(s)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def get_q(self, state):\n",
        "        \"\"\"\n",
        "        Compute Q function for a states\n",
        "        \"\"\"\n",
        "\n",
        "        state = state.flatten()\n",
        "        state_tensor = torch.tensor(state).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            output = self.q_net.forward(state_tensor) # shape (1,  n_actions)\n",
        "        return output.numpy()[0]  # shape  (n_actions)\n",
        "\n",
        "    def decrease_epsilon(self):\n",
        "        self.epsilon = self.epsilon_min + (self.epsilon_start - self.epsilon_min) * (\n",
        "                        np.exp(-1. * self.n_eps / self.decrease_epsilon_factor ) )\n",
        "\n",
        "    def reset(self):\n",
        "        hidden_size = 1024\n",
        "        obs_size = 1\n",
        "        for i in range(len(self.observation_space.shape)):\n",
        "          obs_size *= self.observation_space.shape[i]\n",
        "        n_actions = self.action_space.n\n",
        "\n",
        "        self.buffer = ReplayBuffer(self.buffer_capacity)\n",
        "        self.q_net =  Net(obs_size, hidden_size, n_actions).to(device)\n",
        "        self.target_net = Net(obs_size, hidden_size, n_actions).to(device)\n",
        "\n",
        "        self.loss_function = nn.MSELoss()\n",
        "        self.optimizer = optim.AdamW(params=self.q_net.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        self.epsilon = self.epsilon_start\n",
        "        self.n_steps = 0\n",
        "        self.n_eps = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21ac7a03",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "21ac7a03"
      },
      "outputs": [],
      "source": [
        "class RandomAgent:\n",
        "    def __init__(self, observation_space, action_space):\n",
        "        self.action_space = action_space\n",
        "        return\n",
        "\n",
        "    def get_action(self, state, *args):\n",
        "        return self.action_space.sample()\n",
        "\n",
        "    def update(self, *data):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae9698b8",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ae9698b8"
      },
      "outputs": [],
      "source": [
        "class DQN_SkeletonI(DQN_Skeleton):\n",
        "    def get_action(self, state, epsilon=None):\n",
        "        \"\"\"\n",
        "            ** Solution **\n",
        "\n",
        "            Return action according to an epsilon-greedy exploration policy\n",
        "        \"\"\"\n",
        "        if epsilon is None:\n",
        "            epsilon = self.epsilon\n",
        "\n",
        "        if np.random.rand() < epsilon:\n",
        "            return self.action_space.sample()\n",
        "        else:\n",
        "            return np.argmax(self.get_q(state))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e15cae2",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9e15cae2"
      },
      "outputs": [],
      "source": [
        "def eval_agent(agent, env, n_sim=5):\n",
        "    \"\"\"\n",
        "    ** Solution **\n",
        "\n",
        "    Monte Carlo evaluation of DQN agent.\n",
        "\n",
        "    Repeat n_sim times:\n",
        "        * Run the DQN policy until the environment reaches a terminal state (= one episode)\n",
        "        * Compute the sum of rewards in this episode\n",
        "        * Store the sum of rewards in the episode_rewards array.\n",
        "    \"\"\"\n",
        "    env_copy = deepcopy(env)\n",
        "    episode_rewards = np.zeros(n_sim)\n",
        "    for i in range(n_sim):\n",
        "        state, _ = env_copy.reset()\n",
        "        reward_sum = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.get_action(state, 0)\n",
        "            state, reward, terminated, truncated, _ = env_copy.step(action)\n",
        "            reward_sum += reward\n",
        "            done = terminated or truncated\n",
        "        episode_rewards[i] = reward_sum\n",
        "    return episode_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1226135",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f1226135"
      },
      "outputs": [],
      "source": [
        "class DQN(DQN_SkeletonI):\n",
        "    def update(self, state, action, reward, terminated, next_state):\n",
        "        \"\"\"\n",
        "        ** SOLUTION **\n",
        "        \"\"\"\n",
        "\n",
        "        # add data to replay buffer\n",
        "        self.buffer.push(torch.tensor(state).unsqueeze(0),\n",
        "                           torch.tensor([[action]], dtype=torch.int64),\n",
        "                           torch.tensor([reward]),\n",
        "                           torch.tensor([terminated], dtype=torch.int64),\n",
        "                           torch.tensor(next_state).unsqueeze(0),\n",
        "                          )\n",
        "\n",
        "        if len(self.buffer) < self.batch_size:\n",
        "            return np.inf\n",
        "\n",
        "        # get batch\n",
        "        transitions = self.buffer.sample(self.batch_size)\n",
        "\n",
        "        # Compute loss - TO BE IMPLEMENTED!\n",
        "        # Hint: use the gather method from torch.\n",
        "\n",
        "        state_batch, action_batch, reward_batch, terminated_batch, next_state_batch = tuple(\n",
        "            [torch.cat(data) for data in zip(*transitions)]\n",
        "        )\n",
        "        values  = self.q_net.forward(state_batch).gather(1, action_batch)\n",
        "\n",
        "        # Compute the ideal Q values\n",
        "        with torch.no_grad():\n",
        "            next_state_values = (1 - terminated_batch) * self.target_net(next_state_batch).max(1)[0]\n",
        "            targets = next_state_values * self.gamma + reward_batch\n",
        "\n",
        "        loss = self.loss_function(values, targets.unsqueeze(1).type(torch.float32))\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        #torch.nn.utils.clip_grad_value_(self.q_net.parameters(), 100)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if not((self.n_steps+1) % self.update_target_every):\n",
        "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "        self.decrease_epsilon()\n",
        "\n",
        "        self.n_steps += 1\n",
        "        if terminated:\n",
        "            self.n_eps += 1\n",
        "\n",
        "        return loss.detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "im-eWXPhB_ju",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "im-eWXPhB_ju"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\")\n",
        "\n",
        "config = {\n",
        "    \"observation\": {\n",
        "        \"type\": \"OccupancyGrid\",\n",
        "        \"vehicles_count\": 10,\n",
        "        \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\"],\n",
        "        \"features_range\": {\n",
        "            \"x\": [-100, 100],\n",
        "            \"y\": [-100, 100],\n",
        "            \"vx\": [-20, 20],\n",
        "            \"vy\": [ 00, 20],\n",
        "        },\n",
        "        \"grid_size\": [[-20, 20], [-20, 20]],\n",
        "        \"grid_step\": [5, 5],\n",
        "        \"absolute\": False,\n",
        "    },\n",
        "    \"action\": {\n",
        "        \"type\": \"DiscreteAction\",\n",
        "    },\n",
        "    \"lanes_count\": 3,\n",
        "    \"vehicles_count\": 10,\n",
        "    \"duration\": 20,  # [s]\n",
        "    \"initial_spacing\": 0,\n",
        "    \"collision_reward\": -1,  # The reward received when colliding with a vehicle.\n",
        "    \"right_lane_reward\": 0.6,  # The reward received when driving on the right-most lanes, linearly mapped to\n",
        "    # zero for other lanes.\n",
        "    \"high_speed_reward\": 0.6,  # The reward received when driving at full speed, linearly mapped to zero for\n",
        "    # lower speeds according to config[\"reward_speed_range\"].\n",
        "    \"lane_change_reward\": 0,\n",
        "    \"reward_speed_range\": [\n",
        "        10,\n",
        "        30,\n",
        "    ],  # [m/s] The reward for high speed is mapped linearly from this range to [0, HighwayEnv.HIGH_SPEED_REWARD].\n",
        "    \"simulation_frequency\": 5,  # [Hz]\n",
        "    \"policy_frequency\": 1,  # [Hz]\n",
        "    \"other_vehicles_type\": \"highway_env.vehicle.behavior.IDMVehicle\",\n",
        "    \"screen_width\": 600,  # [px]\n",
        "    \"screen_height\": 150,  # [px]\n",
        "    \"centering_position\": [0.3, 0.5],\n",
        "    \"scaling\": 5.5,\n",
        "    \"show_trajectories\": True,\n",
        "    \"render_agent\": True,\n",
        "    \"offscreen_rendering\": False,\n",
        "    \"disable_collision_checks\": True,\n",
        "}\n",
        "\n",
        "\n",
        "env.unwrapped.configure(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JFbKWxt5uMzY",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JFbKWxt5uMzY",
        "outputId": "e66cce71-ec13-48b7-8af9-e8232b987a8d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 448])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state, _ = env.reset()\n",
        "state = state.flatten()  # Flattening the state at reset for each episode\n",
        "state_tensor = torch.tensor(state).unsqueeze(0)\n",
        "state_tensor.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2EyuZGCUCTY6",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2EyuZGCUCTY6",
        "outputId": "8e961902-3511-4883-cb37-5699b458bf25"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(env.observation_space.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gdTivjG6u7k9",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gdTivjG6u7k9",
        "outputId": "34fb494b-dfa4-427a-c25d-038ac035fbda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "448\n",
            "9\n"
          ]
        }
      ],
      "source": [
        "obs_size = env.observation_space.shape[0] * env.observation_space.shape[1] * env.observation_space.shape[2]\n",
        "n_actions = env.action_space.n\n",
        "print(obs_size)\n",
        "print(n_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JG4QbLjzy-oo",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JG4QbLjzy-oo"
      },
      "outputs": [],
      "source": [
        "hidden_size = 128\n",
        "net = nn.Sequential(\n",
        "    nn.Linear(obs_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),  # Adding dropout for regularization\n",
        "    nn.Linear(hidden_size, hidden_size),  # Adding an extra hidden layer\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),  # Adding dropout for regularization\n",
        "    nn.Linear(hidden_size, n_actions)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86d06cbd",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "86d06cbd",
        "outputId": "7f46d8c2-ec20-49b6-cd37-2872a49de6cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rH4-_poK3RdH",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rH4-_poK3RdH",
        "outputId": "3d68df34-613a-4e8d-a9fb-859d2f07e395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.float32\n",
            "torch.float32\n"
          ]
        }
      ],
      "source": [
        "print(net(state_tensor).dtype)\n",
        "print(state_tensor.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "920b7751",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "920b7751"
      },
      "outputs": [],
      "source": [
        "def train(env, agent, N_episodes, eval_every=10, reward_threshold=300):\n",
        "    total_time = 0\n",
        "    state, _ = env.reset()\n",
        "    losses = []\n",
        "    for ep in range(N_episodes):\n",
        "        done = False\n",
        "        state, _ = env.reset()\n",
        "        state = state.flatten()  # Flattening the state at reset for each episode\n",
        "        while not done:\n",
        "            action = agent.get_action(state, agent.epsilon)  # Make sure to pass epsilon for exploration\n",
        "            #,action = agent.get_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            next_state = next_state.flatten().astype(np.float32)  # Flattening next_state from the environment\n",
        "            loss_val = agent.update(state, action, reward, terminated, next_state)\n",
        "            state = next_state\n",
        "            losses.append(loss_val)\n",
        "            done = terminated or truncated\n",
        "            total_time += 1\n",
        "\n",
        "\n",
        "\n",
        "        if ((ep+1)% eval_every == 0):\n",
        "            rewards = eval_agent(agent, env)\n",
        "            print(\"episode =\", ep+1, \", reward = \", np.mean(rewards), \", loss = \", loss_val)\n",
        "            if np.mean(rewards) >= reward_threshold:\n",
        "                break\n",
        "\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ziKge5LNpufB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziKge5LNpufB",
        "outputId": "03c91ba4-26e3-43db-bdea-a51f59026c8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode = 10 , reward =  7.28125 , loss =  0.047213484\n",
            "episode = 20 , reward =  6.225 , loss =  0.062425558\n",
            "episode = 30 , reward =  7.1875 , loss =  0.05611602\n",
            "episode = 40 , reward =  7.030844082092949 , loss =  0.07104853\n",
            "episode = 50 , reward =  3.775 , loss =  0.073089175\n",
            "episode = 60 , reward =  3.05 , loss =  0.12214478\n",
            "episode = 70 , reward =  1.5625 , loss =  0.10989931\n",
            "episode = 80 , reward =  3.040806403610733 , loss =  0.08195495\n",
            "episode = 90 , reward =  2.393070905545266 , loss =  0.07984654\n",
            "episode = 100 , reward =  2.29375 , loss =  0.08992642\n",
            "episode = 110 , reward =  3.168344082092948 , loss =  0.17696407\n",
            "episode = 120 , reward =  4.547956968515089 , loss =  0.09148943\n",
            "episode = 130 , reward =  3.3979015982943084 , loss =  0.10553448\n",
            "episode = 140 , reward =  3.3160382279329736 , loss =  0.07985937\n",
            "episode = 150 , reward =  3.6976564036107336 , loss =  0.14551772\n",
            "episode = 160 , reward =  3.3102515982943084 , loss =  0.085222036\n",
            "episode = 170 , reward =  2.369862 , loss =  0.12902696\n",
            "episode = 180 , reward =  3.5607599999999997 , loss =  0.11620574\n",
            "episode = 190 , reward =  3.6109373988714024 , loss =  0.11065441\n",
            "episode = 200 , reward =  6.4796 , loss =  0.12617287\n",
            "episode = 210 , reward =  2.649164 , loss =  0.07714286\n",
            "episode = 220 , reward =  3.04435 , loss =  0.107680574\n",
            "episode = 230 , reward =  2.302 , loss =  0.13354866\n",
            "episode = 240 , reward =  3.1546 , loss =  0.14494236\n",
            "episode = 250 , reward =  2.718344082092948 , loss =  0.18174344\n",
            "episode = 260 , reward =  3.1375 , loss =  0.10951045\n",
            "episode = 270 , reward =  5.3581568 , loss =  0.14164612\n",
            "episode = 280 , reward =  2.8096767999999996 , loss =  0.12899329\n",
            "episode = 290 , reward =  4.2201036 , loss =  0.12752183\n",
            "episode = 300 , reward =  6.33185 , loss =  0.1371629\n",
            "episode = 310 , reward =  5.6 , loss =  0.16478944\n",
            "episode = 320 , reward =  5.5375 , loss =  0.10466303\n",
            "episode = 330 , reward =  3.9672600000000005 , loss =  0.24200481\n",
            "episode = 340 , reward =  5.806612807221467 , loss =  0.15410936\n",
            "episode = 350 , reward =  3.795183365590133 , loss =  0.1443592\n",
            "episode = 360 , reward =  3.3449799999999996 , loss =  0.13570787\n",
            "episode = 370 , reward =  3.4929 , loss =  0.33369416\n",
            "episode = 380 , reward =  3.1187407638160183 , loss =  0.25616747\n",
            "episode = 390 , reward =  2.76875 , loss =  0.18675593\n",
            "episode = 400 , reward =  4.143504161598309 , loss =  0.23199835\n",
            "episode = 410 , reward =  6.80625 , loss =  0.21310318\n",
            "episode = 420 , reward =  5.6266 , loss =  0.3998867\n",
            "episode = 430 , reward =  6.89025 , loss =  0.31205192\n",
            "episode = 440 , reward =  6.65 , loss =  0.2427474\n",
            "episode = 450 , reward =  6.30725 , loss =  0.25730765\n",
            "episode = 460 , reward =  4.696617002977302 , loss =  0.17259267\n",
            "episode = 470 , reward =  6.75625 , loss =  0.24109265\n",
            "episode = 480 , reward =  3.3904999999999994 , loss =  0.34470946\n",
            "episode = 490 , reward =  6.6506 , loss =  0.2242705\n",
            "episode = 500 , reward =  5.99375 , loss =  0.17982614\n",
            "episode = 510 , reward =  2.25090593955836 , loss =  0.23181854\n",
            "episode = 520 , reward =  4.41875 , loss =  0.2580227\n",
            "episode = 530 , reward =  5.6 , loss =  0.41217035\n",
            "episode = 540 , reward =  2.40625 , loss =  0.7386221\n",
            "episode = 550 , reward =  6.1375 , loss =  0.3435945\n",
            "episode = 560 , reward =  5.5625 , loss =  0.41007698\n",
            "episode = 570 , reward =  6.31875 , loss =  0.29090813\n",
            "episode = 580 , reward =  6.85625 , loss =  0.22422746\n",
            "episode = 590 , reward =  5.9425 , loss =  0.41228154\n",
            "episode = 600 , reward =  6.375 , loss =  0.20633575\n",
            "episode = 610 , reward =  2.324594082092948 , loss =  0.7785586\n",
            "episode = 620 , reward =  3.80625 , loss =  0.16713393\n",
            "episode = 630 , reward =  4.2517199395583605 , loss =  0.5436113\n",
            "episode = 640 , reward =  3.893740763816018 , loss =  0.32395956\n",
            "episode = 650 , reward =  4.829284728862835 , loss =  0.30956715\n",
            "episode = 660 , reward =  4.2 , loss =  0.32246414\n",
            "episode = 670 , reward =  2.8572499999999996 , loss =  0.28522584\n",
            "episode = 680 , reward =  3.800076687458437 , loss =  0.5206916\n",
            "episode = 690 , reward =  5.20685 , loss =  0.29588282\n",
            "episode = 700 , reward =  5.24475 , loss =  0.31750154\n",
            "episode = 710 , reward =  4.975 , loss =  0.19030806\n",
            "episode = 720 , reward =  2.65258 , loss =  0.2772493\n",
            "episode = 730 , reward =  6.85 , loss =  0.7221154\n",
            "episode = 740 , reward =  3.25 , loss =  0.39791644\n",
            "episode = 750 , reward =  2.84185 , loss =  0.42565563\n",
            "episode = 760 , reward =  4.61 , loss =  0.29798582\n",
            "episode = 770 , reward =  4.728306403610733 , loss =  1.0421643\n",
            "episode = 780 , reward =  6.29137549102264 , loss =  0.24612962\n",
            "episode = 790 , reward =  3.140164589342124 , loss =  0.28734487\n",
            "episode = 800 , reward =  7.075 , loss =  0.29428864\n",
            "episode = 810 , reward =  3.5018299999999996 , loss =  0.42988813\n",
            "episode = 820 , reward =  2.5945639999999996 , loss =  0.5432205\n",
            "episode = 830 , reward =  4.236844082092948 , loss =  0.26069865\n",
            "episode = 840 , reward =  3.432362 , loss =  0.27932012\n",
            "episode = 850 , reward =  3.9401645893421238 , loss =  0.29623342\n",
            "episode = 860 , reward =  7.5 , loss =  0.4122669\n",
            "episode = 870 , reward =  3.74375 , loss =  0.43068928\n"
          ]
        }
      ],
      "source": [
        "action_space = env.action_space\n",
        "observation_space = env.observation_space\n",
        "\n",
        "batch_size = 256\n",
        "buffer_capacity = 10_000\n",
        "update_target_every = 32\n",
        "\n",
        "gamma = 0.99\n",
        "batch_size = 64\n",
        "buffer_capacity = 10_000\n",
        "update_target_every = 512\n",
        "\n",
        "epsilon_start = 0.99\n",
        "decrease_epsilon_factor = 1000 # 300\n",
        "epsilon_min = 0.03\n",
        "\n",
        "learning_rate = 1e-3 \n",
        "\n",
        "arguments = (action_space,\n",
        "            observation_space,\n",
        "            gamma,\n",
        "            batch_size,\n",
        "            buffer_capacity,\n",
        "            update_target_every,\n",
        "            epsilon_start,\n",
        "            decrease_epsilon_factor,\n",
        "            epsilon_min,\n",
        "            learning_rate,\n",
        "        )\n",
        "\n",
        "N_episodes = int(5e2)\n",
        "\n",
        "agent = DQN(*arguments)\n",
        "\n",
        "\n",
        "# Run the training loop\n",
        "losses = train(env, agent, N_episodes)\n",
        "\n",
        "plt.plot(losses)\n",
        "\n",
        "# Evaluate the final policy\n",
        "rewards = eval_agent(agent, env, 20)\n",
        "print(\"\")\n",
        "print(\"mean reward after training = \", np.mean(rewards))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "898bfde9",
      "metadata": {
        "id": "898bfde9"
      },
      "outputs": [],
      "source": [
        "print(\"Rewards after training = \", eval_agent(agent, env))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P3FixzsyBj7a",
      "metadata": {
        "id": "P3FixzsyBj7a"
      },
      "outputs": [],
      "source": [
        "run_one_episode(env, agent, display=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
